"""
Kalshi Atomic API Ingestion - Reliable Complete Data Processing
Strategy: Process each endpoint atomically from start to finish with comprehensive logging
"""

import json
import os
import requests
import time
import hashlib
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class KalshiAtomicIngestion:
    def __init__(self, verify_ssl=False):
        self.base_url = "https://api.elections.kalshi.com/trade-api/v2"
        self.verify_ssl = verify_ssl
        self.available_endpoints = {}
        self.output_folder = self.create_output_folder()
        self.retry_attempts = 3
        self.adaptive_delay = 0.1
        self.requests_this_minute = 0
        self.minute_start = time.time()
        self.setup_logging()
        self.load_discovered_endpoints()
    
    def create_output_folder(self) -> str:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        folder_name = f"kalshi_atomic_ingestion_{timestamp}"
        
        if not os.path.exists(folder_name):
            os.makedirs(folder_name)
            print(f"ğŸ“ Created output folder: {folder_name}")
        
        return folder_name
        
    def setup_logging(self):
        log_filename = os.path.join(self.output_folder, 'atomic_ingestion.log')
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_filename),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"ğŸš€ ATOMIC INGESTION SESSION START - Output: {self.output_folder}")
    
    def get_organized_path(self, endpoint: str, record_index: int = None) -> str:
        safe_endpoint = endpoint.replace('/', '_').replace('{', '').replace('}', '').strip('_')
        
        endpoint_folder = os.path.join(self.output_folder, safe_endpoint)
        if not os.path.exists(endpoint_folder):
            os.makedirs(endpoint_folder)
        
        if record_index is not None:
            batch_size = 1000
            batch_number = record_index // batch_size
            batch_folder = os.path.join(endpoint_folder, f"batch_{batch_number:04d}")
            if not os.path.exists(batch_folder):
                os.makedirs(batch_folder)
            return batch_folder
        
        return endpoint_folder
        
    def load_discovered_endpoints(self):
        discovery_files = [f for f in os.listdir('.') if f.startswith('kalshi_api_discovery_') and f.endswith('.json')]
        
        if not discovery_files:
            self.logger.warning("âš ï¸ No API discovery file found. Using default endpoints.")
            self.available_endpoints = self.get_default_endpoints()
            return
        
        latest_file = max(discovery_files)
        
        try:
            with open(latest_file, 'r') as f:
                discovery_data = json.load(f)
            
            for endpoint_data in discovery_data.get('public_endpoints', []):
                self.available_endpoints[endpoint_data['endpoint']] = {
                    'description': endpoint_data['description'],
                    'capabilities': endpoint_data.get('capabilities', []),
                    'method': endpoint_data.get('method', 'GET')
                }
            
            self.logger.info(f"âœ… Loaded {len(self.available_endpoints)} endpoints from {latest_file}")
            
        except Exception as e:
            self.logger.error(f"âš ï¸ Error loading discovery file: {e}")
            self.available_endpoints = self.get_default_endpoints()
    
    def get_default_endpoints(self):
        return {
            '/markets': {'description': 'Markets', 'capabilities': ['Supports pagination'], 'method': 'GET'},
            '/events': {'description': 'Events', 'capabilities': ['Supports pagination'], 'method': 'GET'},
            '/series': {'description': 'Series', 'capabilities': ['JSON response'], 'method': 'GET'}
        }
    
    def adaptive_rate_limit(self):
        current_time = time.time()
        
        if current_time - self.minute_start >= 60:
            self.requests_this_minute = 0
            self.minute_start = current_time
            
        if self.requests_this_minute >= 50:
            time.sleep(1)
        else:
            time.sleep(self.adaptive_delay)
            
        self.requests_this_minute += 1
    
    def make_request(self, endpoint: str, query_params: Optional[Dict] = None) -> Optional[Dict]:
        url = f"{self.base_url}{endpoint}"
        
        for attempt in range(self.retry_attempts):
            try:
                self.adaptive_rate_limit()
                
                response = requests.get(
                    url,
                    params=query_params or {},
                    verify=self.verify_ssl,
                    timeout=30
                )
                
                if response.status_code == 200:
                    self.adaptive_delay = max(0.05, self.adaptive_delay * 0.95)
                    return response.json()
                elif response.status_code == 429:
                    self.adaptive_delay = min(2.0, self.adaptive_delay * 2)
                    wait_time = min(2 ** attempt * 3, 30)
                    self.logger.warning(f"â³ Rate limited, waiting {wait_time}s...")
                    time.sleep(wait_time)
                    continue
                else:
                    self.logger.warning(f"âš ï¸ API returned status {response.status_code}")
                    return None
                    
            except Exception as e:
                self.logger.error(f"âŒ Request failed (attempt {attempt + 1}): {e}")
                if attempt < self.retry_attempts - 1:
                    time.sleep(min(2 ** attempt, 10))
                    continue
                return None
                
        return None
    
    def process_endpoint_atomically(self, endpoint: str) -> Dict[str, Any]:
        """Process a single endpoint completely from start to finish"""
        
        start_time = time.time()
        self.logger.info(f"ğŸ¯ STARTING ATOMIC PROCESSING: {endpoint}")
        print(f"\\nğŸ¯ Processing {endpoint} atomically...")
        
        endpoint_info = self.available_endpoints.get(endpoint, {})
        supports_pagination = 'Supports pagination' in endpoint_info.get('capabilities', [])
        
        # Get appropriate limit for this endpoint
        if endpoint == '/events':
            limit = 100
        elif endpoint == '/series':
            limit = 200
        else:
            limit = 1000
            
        all_records = []
        seen_hashes = set()
        total_duplicates = 0
        page = 0
        cursor = None
        
        print(f"   ğŸ“Š Strategy: {'Paginated' if supports_pagination else 'Single request'}")
        print(f"   ğŸ“Š Limit per page: {limit}")
        print(f"   ğŸ“Š Deduplication: Content hash based")
        
        if supports_pagination:
            # Paginated processing
            while True:
                page += 1
                query_params = {'limit': limit}
                if cursor:
                    query_params['cursor'] = cursor
                
                self.logger.info(f"ğŸ“„ Fetching page {page} for {endpoint}")
                data = self.make_request(endpoint, query_params)
                
                if not data:
                    self.logger.error(f"âŒ Failed to fetch page {page}")
                    break
                
                # Extract items based on endpoint type
                if 'markets' in data:
                    page_items = data['markets']
                elif 'events' in data:
                    page_items = data['events']
                elif 'series' in data:
                    page_items = data['series']
                else:
                    self.logger.warning(f"âš ï¸ Unexpected data structure in {endpoint}")
                    break
                
                if not page_items:
                    self.logger.info(f"ğŸ“„ Empty page {page} - ending pagination")
                    break
                
                # Process items and deduplicate
                page_duplicates = 0
                for item in page_items:
                    item_hash = hashlib.sha256(json.dumps(item, sort_keys=True).encode()).hexdigest()[:16]
                    if item_hash not in seen_hashes:
                        all_records.append(item)
                        seen_hashes.add(item_hash)
                    else:
                        page_duplicates += 1
                        total_duplicates += 1
                
                self.logger.info(f"âœ… Page {page}: {len(page_items)} items, {page_duplicates} duplicates, {len(all_records)} total unique")
                print(f"   ğŸ“„ Page {page}: {len(page_items)} items, {page_duplicates} duplicates â†’ {len(all_records):,} total unique")
                
                # Check for next page
                cursor = data.get('cursor')
                if not cursor:
                    self.logger.info(f"ğŸ No more pages - pagination complete")
                    break
                    
                # Progress update every 10 pages
                if page % 10 == 0:
                    elapsed = time.time() - start_time
                    rate = len(all_records) / elapsed if elapsed > 0 else 0
                    print(f"   ğŸ”„ Progress: {page} pages, {len(all_records):,} unique records, {rate:.0f} rec/sec")
        else:
            # Single request processing
            self.logger.info(f"ğŸ“„ Single request for {endpoint}")
            data = self.make_request(endpoint)
            
            if data and 'series' in data:
                all_records = data['series']
                self.logger.info(f"âœ… Retrieved {len(all_records)} records")
            else:
                self.logger.error(f"âŒ Failed to retrieve data from {endpoint}")
        
        # Save all records
        saved_count = 0
        if all_records:
            saved_count = self.save_endpoint_records(endpoint, all_records)
        
        # Final statistics
        end_time = time.time()
        duration = end_time - start_time
        
        result = {
            'endpoint': endpoint,
            'success': len(all_records) > 0,
            'records_processed': len(all_records),
            'files_saved': saved_count,
            'duplicates_found': total_duplicates,
            'pages_processed': page,
            'duration_seconds': duration,
            'records_per_second': len(all_records) / duration if duration > 0 else 0
        }
        
        self.logger.info(f"ğŸ¯ ATOMIC PROCESSING COMPLETE: {endpoint}")
        self.logger.info(f"ğŸ“Š Results: {len(all_records)} unique records, {saved_count} files saved, {duration:.1f}s")
        
        print(f"\\nğŸ¯ {endpoint} COMPLETE:")
        print(f"   âœ… Unique records: {len(all_records):,}")
        print(f"   ğŸ’¾ Files saved: {saved_count:,}")
        print(f"   ğŸ”„ Duplicates: {total_duplicates:,}")
        print(f"   ğŸ“„ Pages: {page}")
        print(f"   â±ï¸  Duration: {duration:.1f}s")
        print(f"   ğŸš€ Speed: {result['records_per_second']:.0f} records/sec")
        
        return result
    
    def save_endpoint_records(self, endpoint: str, records: List[Dict]) -> int:
        """Save all records for an endpoint to organized files"""
        
        if not records:
            return 0
            
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        safe_endpoint = endpoint.replace('/', '_').replace('{', '').replace('}', '').strip('_')
        saved_count = 0
        
        self.logger.info(f"ğŸ’¾ SAVING {len(records)} records for {endpoint}")
        print(f"   ğŸ’¾ Saving {len(records):,} records to files...")
        
        for i, record in enumerate(records):
            record_hash = hashlib.sha256(json.dumps(record, sort_keys=True).encode()).hexdigest()[:16]
            record_id = record.get('ticker') or record.get('id') or f"record_{i+1:04d}"
            filename = f"kalshi_{safe_endpoint}_{record_id}_{timestamp}.json"
            filename = filename.replace('/', '_').replace('__', '_')
            
            batch_folder = self.get_organized_path(endpoint, i)
            filepath = os.path.join(batch_folder, filename)
            
            output_data = {
                'endpoint': endpoint,
                'ingestion_time': timestamp,
                'record_id': record_id,
                'record_hash': record_hash,
                'batch_info': {
                    'record_index': i,
                    'batch_number': i // 1000,
                    'total_records': len(records)
                },
                'data': record
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
            saved_count += 1
            
            if (i + 1) % 1000 == 0:
                self.logger.info(f"ğŸ’¾ Saved batch {i // 1000}: records {i - 999}-{i}")
                print(f"   ğŸ’¾ Batch {i // 1000} saved: records {i - 999}-{i}")
        
        self.logger.info(f"âœ… SAVE COMPLETE: {endpoint} - {saved_count} files")
        print(f"   âœ… All {saved_count:,} files saved")
        return saved_count
    
    def run_atomic_bulk_ingestion(self):
        """Run complete atomic ingestion for all endpoints"""
        
        session_start = time.time()
        self.logger.info(f"ğŸš€ ATOMIC BULK INGESTION SESSION START")
        print(f"ğŸš€ KALSHI ATOMIC BULK INGESTION")
        print(f"=" * 60)
        print(f"Strategy: Atomic endpoint processing")
        print(f"Endpoints: {len(self.available_endpoints)}")
        print(f"Output folder: {self.output_folder}")
        print(f"=" * 60)
        
        results = []
        successful_endpoints = []
        failed_endpoints = []
        total_records = 0
        total_files = 0
        
        for i, (endpoint, endpoint_info) in enumerate(self.available_endpoints.items(), 1):
            print(f"\\nğŸ“Š ENDPOINT {i}/{len(self.available_endpoints)}: {endpoint}")
            print(f"   ğŸ“ Description: {endpoint_info['description']}")
            
            # Skip parameterized endpoints
            if '{' in endpoint:
                self.logger.info(f"â­ï¸ Skipping parameterized endpoint: {endpoint}")
                print(f"   â­ï¸ Skipping (requires parameters)")
                continue
            
            try:
                result = self.process_endpoint_atomically(endpoint)
                results.append(result)
                
                if result['success']:
                    successful_endpoints.append(endpoint)
                    total_records += result['records_processed']
                    total_files += result['files_saved']
                else:
                    failed_endpoints.append(endpoint)
                    
            except Exception as e:
                self.logger.error(f"ğŸ’¥ Fatal error processing {endpoint}: {e}")
                print(f"   ğŸ’¥ Fatal error: {e}")
                failed_endpoints.append(endpoint)
        
        # Session summary
        session_duration = time.time() - session_start
        
        summary = {
            'session_start': datetime.fromtimestamp(session_start).isoformat(),
            'session_duration_seconds': session_duration,
            'total_endpoints_processed': len(results),
            'successful_endpoints': successful_endpoints,
            'failed_endpoints': failed_endpoints,
            'total_unique_records': total_records,
            'total_files_saved': total_files,
            'endpoint_results': results
        }
        
        # Save session summary
        summary_file = os.path.join(self.output_folder, f'atomic_session_summary_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json')
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        # Final output
        print(f"\\nğŸ‰ ATOMIC INGESTION SESSION COMPLETE")
        print(f"=" * 60)
        print(f"ğŸ“Š Total records processed: {total_records:,}")
        print(f"ğŸ’¾ Total files saved: {total_files:,}")
        print(f"âœ… Successful endpoints: {len(successful_endpoints)}")
        print(f"âŒ Failed endpoints: {len(failed_endpoints)}")
        print(f"â±ï¸  Total session time: {session_duration:.1f} seconds")
        print(f"ğŸš€ Average processing rate: {total_records/session_duration:.0f} records/second")
        print(f"ğŸ“ Output folder: {self.output_folder}")
        print(f"ğŸ“‹ Session summary: {summary_file}")
        
        self.logger.info(f"ğŸ‰ ATOMIC INGESTION SESSION COMPLETE")
        self.logger.info(f"ğŸ“Š Final stats: {total_records} records, {total_files} files, {session_duration:.1f}s")
        
        return summary

def main():
    ingestion = KalshiAtomicIngestion()
    ingestion.run_atomic_bulk_ingestion()

if __name__ == "__main__":
    main()